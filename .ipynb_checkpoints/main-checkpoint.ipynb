{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9333e6b9-e7c7-47c8-9eed-049bde4ff570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sneha Dubey\n",
    "# Dr. Fang\n",
    "# CSEN 346\n",
    "# 13 Jun 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6610b597-9c84-4e1e-82d3-e3c3e66c2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e489d16-0a37-4be4-b8df-8a466e34b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load and preprocess LIAR dataset ===\n",
    "def load_liar_split(fname):\n",
    "    df = pd.read_csv(fname, sep='\\t', header=None, names=[\n",
    "        \"id\",\"label\",\"statement\",\"subject\",\"speaker\",\"job\",\"state\",\"party\",\n",
    "        \"barely_true_cnt\",\"false_cnt\",\"half_true_cnt\",\"mostly_true_cnt\",\"pants_fire_cnt\",\"context\"\n",
    "    ])\n",
    "    df[\"party\"] = df[\"party\"].fillna(\"unknown\")\n",
    "    df[\"label3\"] = df[\"label\"].map({\n",
    "        \"pants-fire\": 0, \"false\": 0,\n",
    "        \"barely-true\": 1, \"half-true\": 1,\n",
    "        \"mostly-true\": 2, \"true\": 2\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def get_party_vector(party):\n",
    "    if isinstance(party, str):\n",
    "        return {\n",
    "            'democrat': [1, 0, 0],\n",
    "            'republican': [0, 1, 0],\n",
    "            'independent': [0, 0, 1]\n",
    "        }.get(party.lower(), [0, 0, 0])\n",
    "    else:\n",
    "        return [0, 0, 0]\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    cnt = Counter(w for s in texts for w in s.lower().split())\n",
    "    vocab = {\"<pad>\":0, \"<unk>\":1}\n",
    "    for w,f in cnt.items():\n",
    "        if f >= min_freq:\n",
    "            vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "class LiarFancyDataset(Dataset):\n",
    "    def __init__(self, df, vocab, maxlen=50):\n",
    "        self.vocab = vocab\n",
    "        self.texts = df[\"statement\"].tolist()\n",
    "        self.labels = df[\"label3\"].tolist()\n",
    "        self.maxlen = maxlen\n",
    "        self.parties = df[\"party\"].tolist()\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        tokens = self.texts[i].lower().split()[:self.maxlen]\n",
    "        idxs = [self.vocab.get(t, self.vocab[\"<unk>\"]) for t in tokens]\n",
    "        length = len(idxs)\n",
    "        idxs += [self.vocab[\"<pad>\"]] * (self.maxlen - length)\n",
    "        meta = get_party_vector(self.parties[i])\n",
    "        return torch.tensor(idxs), length, torch.tensor(meta, dtype=torch.float32), self.labels[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5bc3f93-22cb-4cc2-8006-8d206b818f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Attention Layer ===\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
    "        context = torch.sum(weights.unsqueeze(-1) * lstm_out, dim=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0271581b-07ad-4fc2-bbc6-9310ab09c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fancy Liar Model ===\n",
    "class FancyLiarModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, lstm_hidden=128, cnn_channels=64, num_labels=3, meta_size=3):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden, batch_first=True, bidirectional=True)\n",
    "        self.attn = Attention(lstm_hidden)\n",
    "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden*2 + cnn_channels + meta_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths, meta):\n",
    "        emb = self.emb(x)\n",
    "        emb = self.emb_dropout(emb)\n",
    "        cnn_feat = self.pool(self.conv(emb.transpose(1,2))).squeeze(-1)\n",
    "        packed = pack_padded_sequence(emb, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        attn_feat = self.attn(out)\n",
    "        combined = torch.cat([attn_feat, cnn_feat, meta], dim=1)\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf79fd16-21fa-4c63-8ac7-3ebd4eb9bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training Setup ===\n",
    "def train_epoch(model, train_dl, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for x, lengths, meta, y in train_dl:\n",
    "        x = torch.stack(x).to(device)\n",
    "        lengths = list(lengths)\n",
    "        meta = torch.stack(meta).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, lengths, meta)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n",
    "def eval_epoch(model, val_dl, criterion, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, lengths, meta, y in val_dl:\n",
    "            x = torch.stack(x).to(device)\n",
    "            lengths = list(lengths)\n",
    "            meta = torch.stack(meta).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "\n",
    "            outputs = model(x, lengths, meta)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(val_dl)\n",
    "    acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    \n",
    "    print(\"=== Confusion Matrix ===\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8668acb1-527d-467e-b8c4-7dd78cdc547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = load_liar_split(\"train.tsv\")\n",
    "df_val = load_liar_split(\"valid.tsv\")\n",
    "\n",
    "# Build vocab\n",
    "vocab = build_vocab(df_train[\"statement\"])\n",
    "\n",
    "# Datasets & Dataloaders\n",
    "train_ds = LiarFancyDataset(df_train, vocab)\n",
    "val_ds = LiarFancyDataset(df_val, vocab)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=lambda b: list(zip(*b)))\n",
    "val_dl = DataLoader(val_ds, batch_size=32, collate_fn=lambda b: list(zip(*b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7bf75cc-8d39-4be4-a7c2-fdc9a7f1a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FancyLiarModel(vocab_size=len(vocab)).to(device)\n",
    "\n",
    "# Compute class weights to fight imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(df_train[\"label3\"]),\n",
    "                                     y=df_train[\"label3\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02668767-ab02-4774-ac67-3495197f4c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3471    0.7520    0.4750       379\n",
      "           1     0.4286    0.0124    0.0240       485\n",
      "           2     0.4388    0.4690    0.4534       420\n",
      "\n",
      "    accuracy                         0.3801      1284\n",
      "   macro avg     0.4048    0.4111    0.3175      1284\n",
      "weighted avg     0.4079    0.3801    0.2976      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[285   2  92]\n",
      " [319   6 160]\n",
      " [217   6 197]]\n",
      "Epoch 1: Train Acc = 0.3719, Val Acc = 0.3801\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4142    0.5989    0.4898       379\n",
      "           1     0.4729    0.1979    0.2791       485\n",
      "           2     0.4447    0.5643    0.4974       420\n",
      "\n",
      "    accuracy                         0.4361      1284\n",
      "   macro avg     0.4439    0.4537    0.4221      1284\n",
      "weighted avg     0.4463    0.4361    0.4127      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[227  46 106]\n",
      " [199  96 190]\n",
      " [122  61 237]]\n",
      "Epoch 2: Train Acc = 0.4194, Val Acc = 0.4361\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3941    0.7071    0.5061       379\n",
      "           1     0.4929    0.2144    0.2989       485\n",
      "           2     0.4758    0.4452    0.4600       420\n",
      "\n",
      "    accuracy                         0.4354      1284\n",
      "   macro avg     0.4543    0.4556    0.4217      1284\n",
      "weighted avg     0.4582    0.4354    0.4128      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[268  36  75]\n",
      " [250 104 131]\n",
      " [162  71 187]]\n",
      "Epoch 3: Train Acc = 0.4547, Val Acc = 0.4354\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4302    0.4960    0.4608       379\n",
      "           1     0.4146    0.3856    0.3996       485\n",
      "           2     0.4722    0.4452    0.4583       420\n",
      "\n",
      "    accuracy                         0.4377      1284\n",
      "   macro avg     0.4390    0.4423    0.4396      1284\n",
      "weighted avg     0.4381    0.4377    0.4369      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[188 120  71]\n",
      " [160 187 138]\n",
      " [ 89 144 187]]\n",
      "Epoch 4: Train Acc = 0.4890, Val Acc = 0.4377\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4472    0.4802    0.4631       379\n",
      "           1     0.4463    0.3938    0.4184       485\n",
      "           2     0.4566    0.4881    0.4718       420\n",
      "\n",
      "    accuracy                         0.4502      1284\n",
      "   macro avg     0.4500    0.4540    0.4511      1284\n",
      "weighted avg     0.4499    0.4502    0.4491      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[182 105  92]\n",
      " [142 191 152]\n",
      " [ 83 132 205]]\n",
      "Epoch 5: Train Acc = 0.5425, Val Acc = 0.4502\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4157    0.5462    0.4721       379\n",
      "           1     0.4428    0.3753    0.4062       485\n",
      "           2     0.4587    0.4095    0.4327       420\n",
      "\n",
      "    accuracy                         0.4369      1284\n",
      "   macro avg     0.4391    0.4437    0.4370      1284\n",
      "weighted avg     0.4400    0.4369    0.4343      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[207  93  79]\n",
      " [179 182 124]\n",
      " [112 136 172]]\n",
      "Epoch 6: Train Acc = 0.5932, Val Acc = 0.4369\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4425    0.4063    0.4237       379\n",
      "           1     0.4199    0.4866    0.4508       485\n",
      "           2     0.4679    0.4167    0.4408       420\n",
      "\n",
      "    accuracy                         0.4400      1284\n",
      "   macro avg     0.4435    0.4365    0.4384      1284\n",
      "weighted avg     0.4423    0.4400    0.4395      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[154 150  75]\n",
      " [125 236 124]\n",
      " [ 69 176 175]]\n",
      "Epoch 7: Train Acc = 0.6452, Val Acc = 0.4400\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4488    0.4512    0.4500       379\n",
      "           1     0.4330    0.3196    0.3677       485\n",
      "           2     0.4257    0.5524    0.4808       420\n",
      "\n",
      "    accuracy                         0.4346      1284\n",
      "   macro avg     0.4358    0.4411    0.4329      1284\n",
      "weighted avg     0.4353    0.4346    0.4290      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[171  92 116]\n",
      " [133 155 197]\n",
      " [ 77 111 232]]\n",
      "Epoch 8: Train Acc = 0.7060, Val Acc = 0.4346\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4269    0.4697    0.4472       379\n",
      "           1     0.4365    0.3753    0.4035       485\n",
      "           2     0.4378    0.4690    0.4529       420\n",
      "\n",
      "    accuracy                         0.4338      1284\n",
      "   macro avg     0.4337    0.4380    0.4346      1284\n",
      "weighted avg     0.4341    0.4338    0.4326      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[178 102  99]\n",
      " [149 182 154]\n",
      " [ 90 133 197]]\n",
      "Epoch 9: Train Acc = 0.7406, Val Acc = 0.4338\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4654    0.3193    0.3787       379\n",
      "           1     0.4279    0.5505    0.4815       485\n",
      "           2     0.4500    0.4286    0.4390       420\n",
      "\n",
      "    accuracy                         0.4424      1284\n",
      "   macro avg     0.4478    0.4328    0.4331      1284\n",
      "weighted avg     0.4462    0.4424    0.4373      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[121 175  83]\n",
      " [ 81 267 137]\n",
      " [ 58 182 180]]\n",
      "Epoch 10: Train Acc = 0.7795, Val Acc = 0.4424\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_acc = train_epoch(model, train_dl, optimizer, criterion, device)\n",
    "    val_acc = eval_epoch(model, val_dl, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accb81fc-49f8-4f62-9d1a-e4e9772d0e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4654    0.3193    0.3787       379\n",
      "           1     0.4279    0.5505    0.4815       485\n",
      "           2     0.4500    0.4286    0.4390       420\n",
      "\n",
      "    accuracy                         0.4424      1284\n",
      "   macro avg     0.4478    0.4328    0.4331      1284\n",
      "weighted avg     0.4462    0.4424    0.4373      1284\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[121 175  83]\n",
      " [ 81 267 137]\n",
      " [ 58 182 180]]\n"
     ]
    }
   ],
   "source": [
    "final_val_acc = eval_epoch(model, val_dl, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4007f-46c9-418f-9976-9254268e7c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c6b0d-090a-4619-ae0c-6fa2766d4ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f4287-1ef7-4cc6-8b29-bfa5935e1d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca4f0e-7d9f-4a40-994d-56f8dda5f9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
